-- create database

CREATE DATABASE SNOWFLAKE_DATAPIPELINE_DB;
use schema snowflake_datapipeline_db.public;


-- create table

create or replace TABLE s3_dataset_table (
    ABOUT VARCHAR(16777216),
    ASSSOCIATED_LOC VARCHAR(16777216),
    ASSOCIATED_ORG VARCHAR(16777216),
    CATEGORY VARCHAR(16777216),
    DOB VARCHAR(16777216),
    IMAGE_URL VARCHAR(16777216),
    REWARD VARCHAR(16777216),
    TITLE VARCHAR(16777216),
    URL VARCHAR(16777216)
);


-- create stage

CREATE OR REPLACE STAGE snowflake_datapipeline_db.public.S3_dataset_stage
    URL = 's3://airflow-snowflake-data-pipeline1/data'
    CREDENTIALS=(AWS_KEY_ID='AKIAZEGGTIXN753UASFJ' AWS_SECRET_KEY='ATqovk6lSs2ANqE6Ivq2n2t/4mPrmZYb4GlhX9dN')
    FILE_FORMAT = (TYPE = 'CSV');

-- creating pipe    


    CREATE OR REPLACE pipe snowflake_datapipeline_db.public.S3_dataset_pipe auto_ingest=true as   
    copy into snowflake_datapipeline_db.public.S3_dataset_table   
    from @snowflake_datapipeline_db.public.S3_dataset_stage1
    FILE_FORMAT = (FIELD_DELIMITER = ',', SKIP_HEADER =1, TYPE = 'CSV', FIELD_OPTIONALLY_ENCLOSED_BY='"');
    
use SNOWFLAKE_DATAPIPELINE_DB;

--COPY_HISTORY
select *
from table(information_schema.copy_history(TABLE_NAME=>'S3_dataset_table', START_TIME=> DATEADD(hours, -1, CURRENT_TIMESTAMP())));

--useful commands
show pipes;
select * from snowflake_datapipeline_db.information_schema.load_HISTORY;
    
select count(*) from "SNOWFLAKE_DATAPIPELINE_DB"."PUBLIC"."S3_DATASET_TABLE";
   
--truncate SNOWFLAKE_DATAPIPELINE_DB.PUBLIC.S3_DATASET_TABLE;
--drop pipe snowflake_datapipeline_db.public.S3_dataset_pipe ;

select system$PIPE_STATUS('SNOWFLAKE_DATAPIPELINE_DB.PUBLIC.S3_DATASET_PIPE');

ALTER PIPE snowflake_datapipeline_db.public.S3_dataset_pipe REFRESH;



